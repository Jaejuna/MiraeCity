{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import librosa\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import json\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### hyperparameter\n",
    "CFG = {\n",
    "    'SR':16000,\n",
    "    'N_MFCC':128, # Melspectrogram 벡터를 추출할 개수\n",
    "    'SEED':42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fixed random seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) # seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### data preprocessing\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_filelist(subfolder, file_extension):\n",
    "    data_path = Path.cwd()/subfolder\n",
    "    \n",
    "    return list(data_path.glob('**/*' + file_extension))\n",
    "\n",
    "root_path = 'C:/Users/user/git/MiraeCity/SR/data/1.Training/label/06.지하철,버스/'\n",
    "\n",
    "# 이 파일이 위치해있는 폴더의 하위폴더 'data'에 있는 확장자명이 '.json'인 모든 파일을 불러옵니다\n",
    "files = get_filelist(root_path+'01.지하철플랫폼','json')\n",
    "\n",
    "# 저장할 데이터 항목의 이름을 입력합니다. json 파일에 적힌 항목(key)과 같아야합니다.\n",
    "column_names = ['dataSet', 'version', 'mediaUrl', 'date', 'typeInfo', 'conversationType', 'speakerNumber', 'speakers', 'dialogs', 'samplingRate', 'recStime', 'recLen', 'recDevice']\n",
    "result = pd.DataFrame(columns=column_names)   \n",
    "\n",
    "for json_file in files:\n",
    "    df = pd.read_json(json_file)\n",
    "    row_data = pd.json_normalize(data=df['row'])\n",
    "    print(row_data.head(2)) #데이터가 잘 불러와지는지 확인하는 출력\n",
    "    \n",
    "    result = pd.concat([result,df])\n",
    "    \n",
    "# 현재 이 파일이 위치한 폴더의 하위 폴더 data 에 'result.csv'로 저장\n",
    "result.to_csv(Path.cwd()/'data'/'01.지하철플랫폼.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataSet', 'version', 'mediaUrl', 'date', 'typeInfo', 'conversationType', 'speakerNumber', 'speakers', 'dialogs', 'samplingRate', 'recStime', 'recLen', 'recDevice']\n"
     ]
    }
   ],
   "source": [
    "datas = json.load(open('C:/Users/user/git/MiraeCity/SR/data/1.Training/label/06.지하철,버스/01.지하철플랫폼/06_01_000817_210811_SD.json', 'r'))\n",
    "\n",
    "keys = [key for key in datas]\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### json to csv\n",
    "rootdir = 'C:/Users/user/git/MiraeCity/SR/data/2.Validation/label/06.지하철,버스/04.버스안'  # Enter your directory here\n",
    "\n",
    "file_list = [f for f in os.scandir(rootdir) if f.is_file() and f.name.endswith('.json')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "        # Flatten 'typeInfo', 'speakers' and 'dialogs' separately\n",
    "        typeInfo_df = json_normalize(json_data, record_path='typeInfo', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        speakers_df = json_normalize(json_data, record_path='speakers', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        dialogs_df = json_normalize(json_data, record_path='dialogs', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        \n",
    "        # Concatenate all data into one DataFrame\n",
    "        dataframes.append(pd.concat([typeInfo_df, speakers_df, dialogs_df], axis=1))\n",
    "\n",
    "# Concatenate all data from different JSON files\n",
    "big_frame = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "big_frame.to_csv('04.버스안.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### csv concat\n",
    "####### train, valid csv todo\n",
    "\n",
    "# list all csv files in the directory\n",
    "csv_dir = 'C:/Users/user/git/MiraeCity/SR/data/2.Validation/label/csv/'\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# read and concatenate all csv files\n",
    "df_list = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(csv_dir, csv_file))\n",
    "    df_list.append(df)\n",
    "\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# write concatenated dataframe to a new csv file\n",
    "final_df.to_csv('concatenated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2044\\2201321066.py:2: DtypeWarning: Columns (0,1,2,6,8,10,13,15,17,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv('C:/Users/user/git/MiraeCity/SR/data/1.Training/label/csv/지하철,버스/02.지하철안.csv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### data preprocessing\n",
    "train_df = pd.read_csv('C:/Users/user/git/MiraeCity/SR/data/1.Training/label/csv/지하철,버스/02.지하철안.csv')\n",
    "test_df = pd.read_csv('C:/Users/user/git/MiraeCity/SR/data/2.Validation/label/csv/지하철,버스/02.지하철안.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### zip 파일 압축 해제\n",
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_filepath, dest_path):\n",
    "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_path)\n",
    "\n",
    "zip_filepath = 'C:/Users/user/git/MiraeCity/SR/data/2.Validation/raw/VS_06.지하철,버스.zip'  # replace with your zip file path\n",
    "dest_path = 'C:/Users/user/git/MiraeCity/SR/data/2.Validation/raw'  # replace with the path where you want to extract files\n",
    "\n",
    "unzip_file(zip_filepath, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### mfcc feature extract function\n",
    "\n",
    "rootdir = 'C:/Users/user/git/MiraeCity/SR/data/1.Training/raw/'\n",
    "\n",
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['mediaUrl'].astype(str)):\n",
    "        full_path = os.path.join(rootdir, path)\n",
    "        try:\n",
    "            y, sr = librosa.load(full_path, sr=CFG['SR'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {full_path} not found.\")\n",
    "            continue\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CFG['N_MFCC'])\n",
    "        features.append({\n",
    "            'mfcc_mean': np.mean(mfcc, axis=1),\n",
    "            'mfcc_max': np.max(mfcc, axis=1),\n",
    "            'mfcc_min': np.min(mfcc, axis=1),\n",
    "        })\n",
    "\n",
    "    mfcc_df = pd.DataFrame(features)\n",
    "    mfcc_mean_df = pd.DataFrame(mfcc_df['mfcc_mean'].tolist(), columns=[f'mfcc_mean_{i}' for i in range(CFG['N_MFCC'])])\n",
    "    mfcc_max_df = pd.DataFrame(mfcc_df['mfcc_max'].tolist(), columns=[f'mfcc_max_{i}' for i in range(CFG['N_MFCC'])])\n",
    "    mfcc_min_df = pd.DataFrame(mfcc_df['mfcc_min'].tolist(), columns=[f'mfcc_min_{i}' for i in range(CFG['N_MFCC'])])\n",
    "\n",
    "    return pd.concat([mfcc_mean_df, mfcc_max_df, mfcc_min_df], axis=1)\n",
    "\n",
    "##### mel feature extract function\n",
    "def get_feature_mel(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['mediaUrl'].astype(str)):\n",
    "        full_path = os.path.join(rootdir, path)\n",
    "        try:\n",
    "            y, sr = librosa.load(full_path, sr=CFG['SR'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {full_path} not found.\")\n",
    "            continue\n",
    "        n_fft = 2048\n",
    "        win_length = 2048\n",
    "        hop_length = 1024\n",
    "        n_mels = 128\n",
    "\n",
    "        D = np.abs(librosa.stft(data, n_fft=n_fft, win_length = win_length, hop_length=hop_length))\n",
    "        mel = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=n_mels, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "        features.append({\n",
    "            'mel_mean': mel.mean(axis=1),\n",
    "            'mel_max': mel.min(axis=1),\n",
    "            'mel_min': mel.max(axis=1),\n",
    "        })\n",
    "    mel_df = pd.DataFrame(features)\n",
    "    mel_mean_df = pd.DataFrame(mel_df['mel_mean'].tolist(), columns=[f'mel_mean_{i}' for i in range(n_mels)])\n",
    "    mel_max_df = pd.DataFrame(mel_df['mel_max'].tolist(), columns=[f'mel_max_{i}' for i in range(n_mels)])\n",
    "    mel_min_df = pd.DataFrame(mel_df['mel_min'].tolist(), columns=[f'mel_min_{i}' for i in range(n_mels)])\n",
    "\n",
    "    return pd.concat([mel_mean_df, mel_max_df, mel_min_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a6f2d6ee7a4e25a34b42cb55d4aa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/user/git/MiraeCity/SR/data/1.Training/raw/06.지하철,버스/02.지하철안/06_01_000154_210727_SD.wav not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2044\\2926547359.py:10: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(full_path, sr=CFG['SR'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2044\\2411479052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mfcc_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mfcc_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_mel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_feature_mel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_mel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_feature_mel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2044\\2926547359.py\u001b[0m in \u001b[0;36mget_mfcc_feature\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mediaUrl'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mfull_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\ntpath.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult_drive\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBytesWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mgenericpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_arg_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'join'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mhasbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             raise TypeError(f'{funcname}() argument must be str, bytes, or '\n\u001b[0m\u001b[0;32m    153\u001b[0m                             f'os.PathLike object, not {s.__class__.__name__!r}') from None\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasstr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasbytes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'float'"
     ]
    }
   ],
   "source": [
    "train_mf = get_mfcc_feature(train_df)\n",
    "test_mf = get_mfcc_feature(test_df)\n",
    "\n",
    "train_mel = get_feature_mel(train_df)\n",
    "test_mel = get_feature_mel(test_df)\n",
    "\n",
    "train_x = pd.concat([train_mel, train_mf], axis=1)\n",
    "test_x = pd.concat([test_mel, test_mf], axis=1)\n",
    "\n",
    "train_y = train_df['label']\n",
    "\n",
    "train_x['label'] = train_df['label']\n",
    "train_data = TabularDataset(train_x)\n",
    "test_data = TabularDataset(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### autogluon\n",
    "label = 'label'\n",
    "eval_metric = 'accuracy'\n",
    "time_limit = 3600 * 24 # 24 hrs\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=label, eval_metric=eval_metric\n",
    ").fit(train_data, presets='best_quality', time_limit=time_limit, ag_args_fit={'num_gpus': 2, 'num_cpus': 32})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
