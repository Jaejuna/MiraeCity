{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import librosa\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import json\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### hyperparameter\n",
    "CFG = {\n",
    "    'SR':16000,\n",
    "    'N_MFCC':128, # Melspectrogram 벡터를 추출할 개수\n",
    "    'SEED':42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fixed random seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) # seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### data preprocessing\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_filelist(subfolder, file_extension):\n",
    "    data_path = Path.cwd()/subfolder\n",
    "    \n",
    "    return list(data_path.glob('**/*' + file_extension))\n",
    "\n",
    "root_path = 'C:/Users/user/git/MiraeCity/SR/data/1.Training/label/06.지하철,버스/'\n",
    "\n",
    "# 이 파일이 위치해있는 폴더의 하위폴더 'data'에 있는 확장자명이 '.json'인 모든 파일을 불러옵니다\n",
    "files = get_filelist(root_path+'01.지하철플랫폼','json')\n",
    "\n",
    "# 저장할 데이터 항목의 이름을 입력합니다. json 파일에 적힌 항목(key)과 같아야합니다.\n",
    "column_names = ['dataSet', 'version', 'mediaUrl', 'date', 'typeInfo', 'conversationType', 'speakerNumber', 'speakers', 'dialogs', 'samplingRate', 'recStime', 'recLen', 'recDevice']\n",
    "result = pd.DataFrame(columns=column_names)   \n",
    "\n",
    "for json_file in files:\n",
    "    df = pd.read_json(json_file)\n",
    "    row_data = pd.json_normalize(data=df['row'])\n",
    "    print(row_data.head(2)) #데이터가 잘 불러와지는지 확인하는 출력\n",
    "    \n",
    "    result = pd.concat([result,df])\n",
    "    \n",
    "# 현재 이 파일이 위치한 폴더의 하위 폴더 data 에 'result.csv'로 저장\n",
    "result.to_csv(Path.cwd()/'data'/'01.지하철플랫폼.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataSet', 'version', 'mediaUrl', 'date', 'typeInfo', 'conversationType', 'speakerNumber', 'speakers', 'dialogs', 'samplingRate', 'recStime', 'recLen', 'recDevice']\n"
     ]
    }
   ],
   "source": [
    "datas = json.load(open('C:/Users/user/git/MiraeCity/SR/data/1.Training/label/06.지하철,버스/01.지하철플랫폼/06_01_000817_210811_SD.json', 'r'))\n",
    "\n",
    "keys = [key for key in datas]\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### json to csv\n",
    "rootdir = 'C:/Users/user/git/MiraeCity/SR/data/2.Validation/label/06.지하철,버스/04.버스안'  # Enter your directory here\n",
    "\n",
    "file_list = [f for f in os.scandir(rootdir) if f.is_file() and f.name.endswith('.json')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "        # Flatten 'typeInfo', 'speakers' and 'dialogs' separately\n",
    "        typeInfo_df = json_normalize(json_data, record_path='typeInfo', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        speakers_df = json_normalize(json_data, record_path='speakers', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        dialogs_df = json_normalize(json_data, record_path='dialogs', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        \n",
    "        # Concatenate all data into one DataFrame\n",
    "        dataframes.append(pd.concat([typeInfo_df, speakers_df, dialogs_df], axis=1))\n",
    "\n",
    "# Concatenate all data from different JSON files\n",
    "big_frame = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "big_frame.to_csv('04.버스안.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### csv concat\n",
    "####### train, valid csv todo\n",
    "\n",
    "# list all csv files in the directory\n",
    "csv_dir = 'C:/Users/user/git/MiraeCity/SR/data/2.Validation/label/csv/'\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# read and concatenate all csv files\n",
    "df_list = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(csv_dir, csv_file))\n",
    "    df_list.append(df)\n",
    "\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# write concatenated dataframe to a new csv file\n",
    "final_df.to_csv('concatenated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18620\\2201321066.py:2: DtypeWarning: Columns (0,1,2,6,8,10,13,15,17,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv('C:/Users/user/git/MiraeCity/SR/data/1.Training/label/csv/지하철,버스/02.지하철안.csv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### data preprocessing\n",
    "train_df = pd.read_csv('C:/Users/user/git/MiraeCity/SR/data/1.Training/label/csv/지하철,버스/02.지하철안.csv')\n",
    "test_df = pd.read_csv('C:/Users/user/git/MiraeCity/SR/data/2.Validation/label/csv/지하철,버스/02.지하철안.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category               object\n",
      "subCategory            object\n",
      "place                  object\n",
      "bgnoisespl            float64\n",
      "avgnoisespl           float64\n",
      "distance              float64\n",
      "dataSet                object\n",
      "version               float64\n",
      "mediaUrl               object\n",
      "date                  float64\n",
      "conversationType       object\n",
      "speakerNumber         float64\n",
      "speaker               float64\n",
      "gender                 object\n",
      "ageGroup              float64\n",
      "dataSet.1              object\n",
      "version.1             float64\n",
      "mediaUrl.1             object\n",
      "date.1                float64\n",
      "conversationType.1     object\n",
      "speakerNumber.1       float64\n",
      "speaker.1               int64\n",
      "speakerText            object\n",
      "startTime               int64\n",
      "endTime                 int64\n",
      "speakTime               int64\n",
      "vocalVolume            object\n",
      "dataSet.2              object\n",
      "version.2             float64\n",
      "mediaUrl.2             object\n",
      "date.2                  int64\n",
      "conversationType.2     object\n",
      "speakerNumber.2         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample_df = pd.read_csv('C:/Users/user/git/MiraeCity/SR/data/1.Training/label/csv/지하철,버스/02.지하철안.csv', nrows=10)\n",
    "print(sample_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### zip 파일 압축 해제\n",
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_filepath, dest_path):\n",
    "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_path)\n",
    "\n",
    "zip_filepath = 'C:/Users/user/git/MiraeCity/SR/data/1.Training/raw/TS2_06.지하철,버스_02.지하철안.zip'  # replace with your zip file path\n",
    "dest_path = 'C:/Users/user/git/MiraeCity/SR/data/1.Training/raw'  # replace with the path where you want to extract files\n",
    "\n",
    "unzip_file(zip_filepath, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### mfcc feature extract function\n",
    "\n",
    "rootdir = 'C:/Users/user/git/MiraeCity/SR/data/2.Validation/raw/'\n",
    "\n",
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['mediaUrl'].astype(str)):\n",
    "        full_path = os.path.join(rootdir, path)\n",
    "        try:\n",
    "            y, sr = librosa.load(full_path, sr=CFG['SR'])\n",
    "        except FileNotFoundError:\n",
    "            #print(f\"File {full_path} not found.\")\n",
    "            continue\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CFG['N_MFCC'])\n",
    "        features.append({\n",
    "            'mfcc_mean': np.mean(mfcc, axis=1),\n",
    "            'mfcc_max': np.max(mfcc, axis=1),\n",
    "            'mfcc_min': np.min(mfcc, axis=1),\n",
    "        })\n",
    "    if not features:  # If features list is empty\n",
    "        print(\"No valid audio files found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    else:\n",
    "        print(\"Found features\")\n",
    "\n",
    "    mfcc_df = pd.DataFrame(features)\n",
    "    mfcc_mean_df = pd.DataFrame(mfcc_df['mfcc_mean'].tolist(), columns=[f'mfcc_mean_{i}' for i in range(CFG['N_MFCC'])])\n",
    "    mfcc_max_df = pd.DataFrame(mfcc_df['mfcc_max'].tolist(), columns=[f'mfcc_max_{i}' for i in range(CFG['N_MFCC'])])\n",
    "    mfcc_min_df = pd.DataFrame(mfcc_df['mfcc_min'].tolist(), columns=[f'mfcc_min_{i}' for i in range(CFG['N_MFCC'])])\n",
    "\n",
    "    return pd.concat([mfcc_mean_df, mfcc_max_df, mfcc_min_df], axis=1)\n",
    "\n",
    "##### mel feature extract function\n",
    "def get_feature_mel(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['mediaUrl'].astype(str)):\n",
    "        full_path = os.path.join(rootdir, path)\n",
    "        try:\n",
    "            y, sr = librosa.load(full_path, sr=CFG['SR'])\n",
    "        except FileNotFoundError:\n",
    "            #print(f\"File {full_path} not found.\")\n",
    "            continue\n",
    "        n_fft = 2048\n",
    "        win_length = 2048\n",
    "        hop_length = 1024\n",
    "        n_mels = 128\n",
    "\n",
    "        D = np.abs(librosa.stft(y, n_fft=n_fft, win_length = win_length, hop_length=hop_length))\n",
    "        mel = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=n_mels, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "        features.append({\n",
    "            'mel_mean': mel.mean(axis=1),\n",
    "            'mel_max': mel.min(axis=1),\n",
    "            'mel_min': mel.max(axis=1),\n",
    "        })\n",
    "        \n",
    "    if not features:  # If features list is empty\n",
    "        print(\"No valid audio files found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    else:\n",
    "        print(\"Found features\")\n",
    "\n",
    "    mel_df = pd.DataFrame(features)\n",
    "    mel_mean_df = pd.DataFrame(mel_df['mel_mean'].tolist(), columns=[f'mel_mean_{i}' for i in range(n_mels)])\n",
    "    mel_max_df = pd.DataFrame(mel_df['mel_max'].tolist(), columns=[f'mel_max_{i}' for i in range(n_mels)])\n",
    "    mel_min_df = pd.DataFrame(mel_df['mel_min'].tolist(), columns=[f'mel_min_{i}' for i in range(n_mels)])\n",
    "\n",
    "    return pd.concat([mel_mean_df, mel_max_df, mel_min_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6e5a6c6fee40ae9ba07e36d09f0b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18620\\1928698577.py:10: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(full_path, sr=CFG['SR'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid audio files found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840ea9328cf349e890545f6d421dbe43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found features\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180e61ec7cbd4fa483ed347e60cc94e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18620\\1928698577.py:39: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(full_path, sr=CFG['SR'])\n",
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid audio files found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7a0ff349c7434cac9987ed96a0bc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found features\n"
     ]
    }
   ],
   "source": [
    "train_mf = get_mfcc_feature(train_df)\n",
    "test_mf = get_mfcc_feature(test_df)\n",
    "\n",
    "train_mel = get_feature_mel(train_df)\n",
    "test_mel = get_feature_mel(test_df)\n",
    "\n",
    "train_x = pd.concat([train_mel, train_mf], axis=1)\n",
    "test_x = pd.concat([test_mel, test_mf], axis=1)\n",
    "\n",
    "train_y = train_df['place']\n",
    "\n",
    "train_x['place'] = train_df['place']\n",
    "test_x['place'] = test_df['place']\n",
    "\n",
    "train_x['place'] = train_df['place'].fillna(method='ffill')\n",
    "test_x['place'] = test_df['place'].fillna(method='ffill')\n",
    "\n",
    "# train_x.dropna(subset=['place'], inplace=True)\n",
    "# test_x.dropna(subset=['place'], inplace=True)\n",
    "\n",
    "train_data = TabularDataset(train_x)\n",
    "test_data = TabularDataset(test_x)\n",
    "\n",
    "# train_data = pd.concat([train_data, test_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### autogluon\n",
    "label = 'place'\n",
    "eval_metric = 'accuracy'\n",
    "time_limit = 3600 * 1 # hrs\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=label, eval_metric=eval_metric\n",
    ").fit(test_data, presets='best_quality', time_limit=time_limit, ag_args_fit={'num_gpus': 0, 'num_cpus': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### leaderboard\n",
    "predictor.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### inference \n",
    "model_to_use = predictor.get_model_best()\n",
    "model_pred = predictor.predict(test_data, model=model_to_use)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
