{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\mm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pysrt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "\n",
    "import json\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### hyperparameter\n",
    "CFG = {\n",
    "    'SR':16000, # sampling rate\n",
    "    'N_MFCC':128, # Melspectrogram 벡터를 추출할 개수\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fixed random seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) # seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [dataSet, version, mediaUrl, date, typeInfo, conversationType, speakerNumber, speakers, dialogs, samplingRate, recStime, recLen, recDevice]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#### data preprocessing\n",
    "def get_filelist(subfolder, file_extension):\n",
    "    data_path = Path.cwd()/subfolder\n",
    "    \n",
    "    return list(data_path.glob('**/*' + file_extension))\n",
    "\n",
    "rootdir = '../JJJ/MM/New_Sample/라벨링데이터/TL/01.가전소음/02.청소기'\n",
    "\n",
    "#### json handling\n",
    "# 이 파일이 위치해있는 폴더의 하위폴더 'data'에 있는 확장자명이 '.json'인 모든 파일을 불러옵니다\n",
    "files = get_filelist(rootdir+ '*' ,'json')\n",
    "\n",
    "# 저장할 데이터 항목의 이름을 입력합니다. json 파일에 적힌 항목(key)과 같아야합니다.\n",
    "column_names = ['dataSet', 'version', 'mediaUrl', 'date', 'typeInfo', 'conversationType', 'speakerNumber', 'speakers', 'dialogs', 'samplingRate', 'recStime', 'recLen', 'recDevice']\n",
    "result = pd.DataFrame(columns=column_names)   \n",
    "\n",
    "for json_file in files:\n",
    "    df = pd.read_json(json_file)\n",
    "    row_data = pd.json_normalize(data=df['row'])\n",
    "    \n",
    "    result = pd.concat([result,df])\n",
    "\n",
    "# 현재 이 파일이 위치한 폴더의 하위 폴더 data 에 'result.csv'로 저장\n",
    "result.to_csv(Path.cwd()/'data'/'json_sample_category.csv', index=None)\n",
    "\n",
    "print(result.head(2)) #데이터가 잘 불러와지는지 확인하는 출력   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### annotation organize -> json to csv\n",
    "rootdir = 'C:/Users/user/git/MiraeCity/JJJ/MM/New_Sample/라벨링데이터/TL/01.가전소음/02.청소기'\n",
    "\n",
    "file_list = [f for f in os.scandir(rootdir) if f.is_file() and f.name.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "        # Flatten 'typeInfo', 'speakers' and 'dialogs' separately\n",
    "        typeInfo_df = json_normalize(json_data, record_path='typeInfo', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        speakers_df = json_normalize(json_data, record_path='speakers', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        dialogs_df = json_normalize(json_data, record_path='dialogs', meta=['dataSet', 'version', 'mediaUrl', 'date', 'conversationType', 'speakerNumber'], errors='ignore')\n",
    "        \n",
    "        # Concatenate all data into one DataFrame\n",
    "        dataframes.append(pd.concat([typeInfo_df, speakers_df, dialogs_df], axis=1))\n",
    "\n",
    "# Concatenate all data from different JSON files\n",
    "total_dataFrame = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "total_dataFrame.to_csv('C:/Users/user/git/MiraeCity/JJJ/MM/notebook/data/json_sample.csv', index=False) \n",
    "\n",
    "\n",
    "##### annotation organize -> srt to csv\n",
    "def srt_to_df(file_path):\n",
    "    subs = pysrt.open(file_path, encoding='utf-8')\n",
    "    data = {\n",
    "        'start': [],\n",
    "        'end': [],\n",
    "        'text': [],\n",
    "    }\n",
    "    \n",
    "    for sub in subs:\n",
    "        data['start'].append(str(sub.start))\n",
    "        data['end'].append(str(sub.end))\n",
    "        data['text'].append(sub.text)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# a list to store each individual dataframe\n",
    "dfs = []\n",
    "\n",
    "# iterate over all srt files in directory\n",
    "for filename in os.listdir(rootdir):\n",
    "    if filename.endswith(\".srt\"):\n",
    "        srt_file_path = os.path.join(rootdir, filename)\n",
    "        \n",
    "        # load the SRT file\n",
    "        df = srt_to_df(srt_file_path)\n",
    "\n",
    "        # add to list of dataframes\n",
    "        dfs.append(df)\n",
    "\n",
    "# concatenate all dataframes into one\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# save the concatenated dataframe to a single CSV file\n",
    "df_all.to_csv('C:/Users/user/git/MiraeCity/JJJ/MM/notebook/data/jsom_sample_srt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc & mel feature extract function\n",
    "##### mfcc feature extract function\n",
    "rootdir = 'C:/Users/user/git/MiraeCity/SR/data/01.데이터/2.Validation/raw/VS_07.터미널/'\n",
    "\n",
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['mediaUrl'].astype(str)):\n",
    "        full_path = os.path.join(rootdir, path)\n",
    "        try:\n",
    "            y, sr = librosa.load(full_path, sr=CFG['SR'])\n",
    "        except FileNotFoundError:\n",
    "            #print(f\"File {full_path} not found.\")\n",
    "            continue\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CFG['N_MFCC'])\n",
    "        features.append({\n",
    "            'mfcc_mean': np.mean(mfcc, axis=1),\n",
    "            'mfcc_max': np.max(mfcc, axis=1),\n",
    "            'mfcc_min': np.min(mfcc, axis=1),\n",
    "        })\n",
    "    if not features:  # If features list is empty\n",
    "        print(\"No valid audio files found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    else:\n",
    "        print(\"Found features\")\n",
    "\n",
    "    mfcc_df = pd.DataFrame(features)\n",
    "    mfcc_mean_df = pd.DataFrame(mfcc_df['mfcc_mean'].tolist(), columns=[f'mfcc_mean_{i}' for i in range(CFG['N_MFCC'])])\n",
    "    mfcc_max_df = pd.DataFrame(mfcc_df['mfcc_max'].tolist(), columns=[f'mfcc_max_{i}' for i in range(CFG['N_MFCC'])])\n",
    "    mfcc_min_df = pd.DataFrame(mfcc_df['mfcc_min'].tolist(), columns=[f'mfcc_min_{i}' for i in range(CFG['N_MFCC'])])\n",
    "\n",
    "    return pd.concat([mfcc_mean_df, mfcc_max_df, mfcc_min_df], axis=1)\n",
    "\n",
    "##### mel feature extract function\n",
    "def get_feature_mel(df):\n",
    "    features = []\n",
    "    for path in tqdm(df['mediaUrl'].astype(str)):\n",
    "        full_path = os.path.join(rootdir, path)\n",
    "        try:\n",
    "            y, sr = librosa.load(full_path, sr=CFG['SR'])\n",
    "        except FileNotFoundError:\n",
    "            #print(f\"File {full_path} not found.\")\n",
    "            continue\n",
    "        n_fft = 2048\n",
    "        win_length = 2048\n",
    "        hop_length = 1024\n",
    "        n_mels = 128\n",
    "\n",
    "        D = np.abs(librosa.stft(y, n_fft=n_fft, win_length = win_length, hop_length=hop_length))\n",
    "        mel = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=n_mels, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "        features.append({\n",
    "            'mel_mean': mel.mean(axis=1),\n",
    "            'mel_max': mel.min(axis=1),\n",
    "            'mel_min': mel.max(axis=1),\n",
    "        })\n",
    "        \n",
    "    if not features:  # If features list is empty\n",
    "        print(\"No valid audio files found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    else:\n",
    "        print(\"Found features\")\n",
    "\n",
    "    mel_df = pd.DataFrame(features)\n",
    "    mel_mean_df = pd.DataFrame(mel_df['mel_mean'].tolist(), columns=[f'mel_mean_{i}' for i in range(n_mels)])\n",
    "    mel_max_df = pd.DataFrame(mel_df['mel_max'].tolist(), columns=[f'mel_max_{i}' for i in range(n_mels)])\n",
    "    mel_min_df = pd.DataFrame(mel_df['mel_min'].tolist(), columns=[f'mel_min_{i}' for i in range(n_mels)])\n",
    "\n",
    "    return pd.concat([mel_mean_df, mel_max_df, mel_min_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying feature extract function \n",
    "train_mf = get_mfcc_feature(train_df)\n",
    "test_mf = get_mfcc_feature(test_df)\n",
    "\n",
    "train_mel = get_feature_mel(train_df)\n",
    "test_mel = get_feature_mel(test_df)\n",
    "\n",
    "train_x = pd.concat([train_mel, train_mf], axis=1)\n",
    "test_x = pd.concat([test_mel, test_mf], axis=1)\n",
    "\n",
    "train_y = train_df['place']\n",
    "\n",
    "#train_x['place'] = train_df['place']\n",
    "#test_x['place'] = test_df['place']\n",
    "\n",
    "train_x['place'] = train_df['place'].fillna(method='ffill')\n",
    "test_x['place'] = test_df['place'].fillna(method='ffill')\n",
    "\n",
    "# train_x.dropna(subset=['place'], inplace=True)\n",
    "# test_x.dropna(subset=['place'], inplace=True)\n",
    "\n",
    "#train_data = TabularDataset(train_x)\n",
    "#test_data = TabularDataset(test_x)\n",
    "\n",
    "# train_data = pd.concat([train_data, test_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess result\n",
    "print(test_data)\n",
    "test_data.to_csv('test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpecTransformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
